{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version information\n",
    "\n",
    "This notebook was created using Python 3.10 and the following package versions:\n",
    "\n",
    "- pandas 2.2.1\n",
    "- numpy 1.26.1\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Data\n",
    "\n",
    "Before moving data, we need to get data from US CDC:\n",
    "\n",
    "`wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://wonder.cdc.gov/nndss/static/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html file from url\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "def get_website(week):\n",
    "  year = 2015\n",
    "  url = f\"https://wonder.cdc.gov/nndss/nndss_weekly_tables_{year}.asp?mmwr_year={year}&mmwr_week={week}&mmwr_table=2I&request=Submit\"\n",
    "\n",
    "  output_file = f'./US/wonder.cdc.gov/nndss/static/{year}/month/{year}_{week}.html'  # Name of the file to save the HTML content\n",
    "\n",
    "  response = requests.get(url)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "      with open(output_file, 'w', encoding='utf-8') as f:\n",
    "          f.write(response.text)\n",
    "  else:\n",
    "      print('Failed to retrieve HTML content. Status code:', response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_website('01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "weeks = list(range(1, 53))\n",
    "# trans weeks from 1 to 01\n",
    "weeks = [f'{week:02}' for week in weeks]\n",
    "\n",
    "num_processes = int(0.1 * multiprocessing.cpu_count())\n",
    "with Pool(processes=num_processes) as pool:\n",
    "  results_1 = pool.map(get_website, weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to recursively search for HTML files in a directory\n",
    "def find_html_files(directory):\n",
    "    html_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                html_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Remove path contains exclude_dir str\n",
    "    \n",
    "    return html_files\n",
    "\n",
    "# Source directory where HTML files are located\n",
    "source_directory = \"./US/wonder.cdc.gov/nndss/static\"\n",
    "destination_directory = './US/AllData'\n",
    "html_files_list = find_html_files(source_directory)\n",
    "\n",
    "# remove end with 'index.html' or web.config.html\n",
    "html_files_list = [x for x in html_files_list if not x.endswith('index.html') and not x.endswith('web.config.html')]\n",
    "\n",
    "# remove path contains annual\n",
    "html_files_list = [x for x in html_files_list if 'annual' not in x]\n",
    "\n",
    "# remove path contains figure or pdf\n",
    "html_files_list = [x for x in html_files_list if 'figure' not in x and 'pdf' not in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text get date information\n",
    "def extract_date_from_title(title):\n",
    "    # Regular expression pattern for matching dates in the specific format\n",
    "    date_pattern = re.compile(r'(\\bJanuary|\\bFebruary|\\bMarch|\\bApril|\\bMay|\\bJune|\\bJuly|\\bAugust|\\bSeptember|\\bOctober|\\bNovember|\\bDecember)\\s+\\d{1,2},\\s+\\d{4}')\n",
    "    # Search for the pattern in the title\n",
    "    match = date_pattern.search(title)\n",
    "    if match:\n",
    "        date_str = match.group(0)\n",
    "        # Convert the date string to a datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "        return date_obj\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# HTMLParser subclass for parsing titles\n",
    "class TitleParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title = None\n",
    "        self.recording = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'title':\n",
    "            self.recording = True\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.recording:\n",
    "            self.title = data\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self.recording = False\n",
    "\n",
    "# Get year and week from file path\n",
    "def get_year_week(file_path):\n",
    "    # Regular expression pattern for matching year and week in the specific format\n",
    "    year_week_pattern = re.compile(r'(\\d{4})-(\\d{2})')\n",
    "    # Search for the pattern in the file path\n",
    "    match = year_week_pattern.search(file_path)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        week = match.group(2)\n",
    "        return year, week\n",
    "    else:\n",
    "        return None, None\n",
    "            \n",
    "# Function to read HTML files and extract data\n",
    "def read_html_files(file_name):\n",
    "    # Read HTML file content\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Extract tables using pandas\n",
    "    html_io = StringIO(html_content)\n",
    "    try:\n",
    "        df_list = pd.read_html(html_io)\n",
    "        # find the table with the most rows\n",
    "        df = max(df_list, key=lambda x: x.shape[1])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading HTML tables from {file_name}: {e}\")\n",
    "        df = None\n",
    "\n",
    "    # Create parser and extract title\n",
    "    parser = TitleParser()\n",
    "    parser.feed(html_content)\n",
    "    title = parser.title\n",
    "    date = extract_date_from_title(title)\n",
    "    year, week = get_year_week(file_name)\n",
    "\n",
    "    return df, date, year, week\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(file_name):\n",
    "    df, date, year, week = read_html_files(file_name)\n",
    "    if df is not None:\n",
    "        # get column names\n",
    "        col_names = df.columns.values\n",
    "        df_names = pd.DataFrame(col_names.tolist())\n",
    "        df_names = df_names.drop(0)\n",
    "        df_names = df_names.reset_index()\n",
    "        df_names['index'] = df_names['index']\n",
    "\n",
    "        # replace column names with column number\n",
    "        df.columns = range(df.shape[1])\n",
    "        df = df.melt(id_vars=[0], value_vars=range(1, df.shape[1]), var_name='State', value_name='Cases')\n",
    "        df = df.rename(columns={0: 'Area'})\n",
    "\n",
    "        # Replace - with 0 in Cases column\n",
    "        # df['Cases'] = df['Cases'].replace('-', '0')\n",
    "        # df['Cases'] = df['Cases'].replace('—', '0')\n",
    "        # df['Cases'] = df['Cases'].replace('', '0')\n",
    "        # df['Cases'] = df['Cases'].astype(int)\n",
    "\n",
    "        # add date year and week column\n",
    "        df['Date'] = date\n",
    "        df['Year'] = year\n",
    "        df['Week'] = week\n",
    "        # convert file_name to url\n",
    "        df['URL'] = file_name.replace('./US/', 'https://')\n",
    "\n",
    "        # merge with df_names by index and State\n",
    "        df = pd.merge(df, df_names, left_on='State', right_on='index', how='left')\n",
    "        df = df.rename(columns={0: 'Disease'})\n",
    "        # add column 2 if not present\n",
    "        if 2 not in df.columns:\n",
    "            df[2] = ''\n",
    "        if 3 not in df.columns:\n",
    "            df[3] = ''\n",
    "        if 1 in df.columns:\n",
    "            df = df[['Area', 'Date', 'Year', 'Week', 'Disease', 1, 2, 3, 'Cases', 'URL']]\n",
    "            return df\n",
    "        else:\n",
    "            pass\n",
    "    data = {'Area': [''], 'Date': [date], 'Year': [year], 'Week': [week], 'Disease': [''], 1: [''], 2: [''], 3: [''], 'Cases': ['No Data'], 'URL': [file_name.replace('./US/', 'https://')]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3645502/1874626083.py:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(results, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "num_processes = int(0.9 * multiprocessing.cpu_count())\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    results = pool.map(clean_data, html_files_list)\n",
    "final_df = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text get date information\n",
    "def extract_date_from_title(title):\n",
    "    # Regular expression pattern for matching dates in the specific format\n",
    "    date_pattern = re.compile(r'(\\bJanuary|\\bFebruary|\\bMarch|\\bApril|\\bMay|\\bJune|\\bJuly|\\bAugust|\\bSeptember|\\bOctober|\\bNovember|\\bDecember)\\s+\\d{1,2},\\s+\\d{4}')\n",
    "    # Search for the pattern in the title\n",
    "    match = date_pattern.search(title)\n",
    "    if match:\n",
    "        date_str = match.group(0)\n",
    "        # Convert the date string to a datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "        return date_obj\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# HTMLParser subclass for parsing titles\n",
    "class TitleParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title = None\n",
    "        self.recording = False\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'title':\n",
    "            self.recording = True\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.recording:\n",
    "            self.title = data\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self.recording = False\n",
    "\n",
    "# Get year and week from file path\n",
    "def get_year_week(file_path):\n",
    "    # Regular expression pattern for matching year and week in the specific format\n",
    "    year_week_pattern = re.compile(r'(\\d{4})_(\\d{2})')\n",
    "    # Search for the pattern in the file path\n",
    "    match = year_week_pattern.search(file_path)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        week = match.group(2)\n",
    "        return year, week\n",
    "    else:\n",
    "        return None, None\n",
    "            \n",
    "# Function to read HTML files and extract data\n",
    "def read_html_files(file_name):\n",
    "    # Read HTML file content\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Extract tables using pandas\n",
    "    html_io = StringIO(html_content)\n",
    "    try:\n",
    "        df_list = pd.read_html(html_io)\n",
    "        # find the table with the most rows\n",
    "        df = max(df_list, key=lambda x: x.shape[1])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading HTML tables from {file_name}: {e}\")\n",
    "        df = None\n",
    "\n",
    "    # Create parser and extract title\n",
    "    parser = TitleParser()\n",
    "    parser.feed(html_content)\n",
    "    title = parser.title\n",
    "    date = extract_date_from_title(title)\n",
    "    year, week = get_year_week(file_name)\n",
    "\n",
    "    return df, date, year, week\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(file_name):\n",
    "    df, date, year, week = read_html_files(file_name)\n",
    "    if df is not None:\n",
    "        # column names as first 3 rows\n",
    "        df_names = df.iloc[:3, :]\n",
    "        df_names = pd.DataFrame(df_names).T\n",
    "        df_names = df_names.reset_index()\n",
    "        df_names['index'] = df_names['index']\n",
    "        # drop first 3 rows\n",
    "        df = df.iloc[3:, :]\n",
    "        # replace column names with column number\n",
    "        df.columns = range(df.shape[1])\n",
    "        df = df.melt(id_vars=[0], value_vars=range(1, df.shape[1]), var_name='State', value_name='Cases')\n",
    "        df = df.rename(columns={0: 'Area'})\n",
    "\n",
    "        # Replace - with 0 in Cases column\n",
    "        # df['Cases'] = df['Cases'].replace('-', '0')\n",
    "        # df['Cases'] = df['Cases'].replace('—', '0')\n",
    "        # df['Cases'] = df['Cases'].replace('', '0')\n",
    "        # df['Cases'] = df['Cases'].astype(int)\n",
    "\n",
    "        # add date year and week column\n",
    "        df['Date'] = date\n",
    "        df['Year'] = year\n",
    "        df['Week'] = week\n",
    "        # convert file_name to url\n",
    "        df['URL'] = file_name.replace('./US/', 'https://')\n",
    "\n",
    "        # merge with df_names by index and State\n",
    "        df = pd.merge(df, df_names, left_on='State', right_on='index', how='left')\n",
    "        df = df.rename(columns={0: 'Disease'})\n",
    "        # add column 2 if not present\n",
    "        if 2 not in df.columns:\n",
    "            df[2] = ''\n",
    "        if 3 not in df.columns:\n",
    "            df[3] = ''\n",
    "        if 1 in df.columns:\n",
    "            df = df[['Area', 'Date', 'Year', 'Week', 'Disease', 1, 2, 3, 'Cases', 'URL']]\n",
    "            return df\n",
    "        else:\n",
    "            pass\n",
    "    data = {'Area': [''], 'Date': [date], 'Year': [year], 'Week': [week], 'Disease': [''], 1: [''], 2: [''], 3: [''], 'Cases': ['No Data'], 'URL': [file_name.replace('./US/', 'https://')]}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = \"./US/month\"\n",
    "html_files_list = find_html_files(source_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = int(0.9 * multiprocessing.cpu_count())\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    results_1 = pool.map(clean_data, html_files_list)\n",
    "final_df_1 = pd.concat(results_1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row bind final_df_1 and final_df\n",
    "final_df = pd.concat([final_df_1, final_df], ignore_index=True)\n",
    "\n",
    "# save to csv\n",
    "final_df.to_csv('./US/AllData.csv', index=False)\n",
    "\n",
    "# Filter Disease contains pertussis\n",
    "pertussis_df = final_df[final_df['Disease'].str.contains('pertussis', case=False)]\n",
    "pertussis_df = pertussis_df[pertussis_df[1].str.contains('Current', case=False)]\n",
    "# filter Area is in Total, United States, UNITED STATES\n",
    "pertussis_df = pertussis_df[pertussis_df['Area'].isin(['Total', 'United States', 'UNITED STATES'])]\n",
    "\n",
    "# Arrange by Year and Week\n",
    "pertussis_df = pertussis_df.sort_values(by=['Date'])\n",
    "# reindex\n",
    "pertussis_df = pertussis_df.reset_index(drop=True)\n",
    "# save to csv\n",
    "pertussis_df.to_csv('./US/pertussis.csv', index=False)\n",
    "# pertussis_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
